{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb\n",
    "# DQN without a frozen target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext memory_profiler\n",
    "# %load_ext line_profiler\n",
    "# %load_ext heat\n",
    "# %load_ext snakeviz\n",
    "# %env OMP_NUM_THREADS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import datetime\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --port=9706 --logdir ./runs\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT:  base_config \tSEED:  324267 \twriter_dir:  ./runs/D3QN_base_config_324267_20190822-210604\n"
     ]
    }
   ],
   "source": [
    "experiment_no = 'base_config'\n",
    "# FROM CONFIG FILE\n",
    "config_path =  './' + experiment_no + '.yaml' # sys.argv[2]\n",
    "config = yaml.safe_load(open(config_path,'r'))\n",
    "\n",
    "seed_value = 324267 # sys.argv[1]\n",
    "\n",
    "# # Writer will output to ./runs/ directory by default\n",
    "writer_dir = './runs/' + config['MODEL_NAME'] + '_' + experiment_no + '_' + str(seed_value) + '_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(writer_dir)\n",
    "print(\"EXPERIMENT: \", experiment_no, \"\\tSEED: \", seed_value, \"\\twriter_dir: \", writer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os \n",
    "import random \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_flush_denormal(True)\n",
    "# %env KMP_AFFINITY=granularity=fine,compact,1,0\n",
    "# %env KMP_BLOCKTIME=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(seed_value) \n",
    "random.seed(seed_value) \n",
    "np.random.seed(seed_value) \n",
    "# tf.random.set_seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# CartPole-v0 Environment\n",
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)\n",
    "env.seed(seed_value);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = config['USE_GPU']\n",
    "\n",
    "# Use CUDA\n",
    "USE_CUDA = torch.cuda.is_available() and USE_GPU\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY BUFFER\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module): #base model\n",
    "    def __init__(self, num_inputs, num_actions, HIDDEN_LAYER_WIDTH):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.action_dim = num_actions\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, HIDDEN_LAYER_WIDTH),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_WIDTH, HIDDEN_LAYER_WIDTH),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER_WIDTH, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        with torch.no_grad():\n",
    "            if random.random() > epsilon:\n",
    "                state   = torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0).to(device)\n",
    "                q_values = self.forward(state)\n",
    "                action  = q_values.max(dim=1)[1].item()\n",
    "            else:\n",
    "                action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, HIDDEN_LAYER_WIDTH):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.action_dim = num_actions\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        with torch.no_grad():\n",
    "            if random.random() > epsilon:\n",
    "                state   = torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0).to(device)\n",
    "                q_values = self.forward(state)\n",
    "                action  = q_values.max(dim=1)[1].item()\n",
    "            else:\n",
    "                action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-greedy exploration\n",
    "\n",
    "epsilon_start = config['EPSILON_START']\n",
    "epsilon_final = config['EPSILON_FINAL']\n",
    "epsilon_decay = config['EPSILON_DECAY']\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f837e9e8da0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuelingDQN(\n",
      "  (feature): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (advantage): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      "  (value): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [32, 128]             640\n",
      "              ReLU-2                  [32, 128]               0\n",
      "            Linear-3                  [32, 128]          16,512\n",
      "              ReLU-4                  [32, 128]               0\n",
      "            Linear-5                    [32, 2]             258\n",
      "            Linear-6                  [32, 128]          16,512\n",
      "              ReLU-7                  [32, 128]               0\n",
      "            Linear-8                    [32, 1]             129\n",
      "================================================================\n",
      "Total params: 34,051\n",
      "Trainable params: 34,051\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.19\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 0.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "if (config['MODEL_NAME']=='D1QN'):\n",
    "    # only one NN for estimating Q-values\n",
    "    model = DQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "elif (config['MODEL_NAME']=='DQN' or config['MODEL_NAME']=='D2QN'):\n",
    "    # one inference model and one target model\n",
    "    model = DQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    target = DQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])\n",
    "    target = target.to(device)\n",
    "\n",
    "elif (config['MODEL_NAME']=='D3QN'):\n",
    "    # one inference model and one target model\n",
    "    model = DuelingDQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    target = DuelingDQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])\n",
    "    target = target.to(device)\n",
    "\n",
    "    \n",
    "else: #default model is D1QN\n",
    "    # only one NN for estimating Q-values\n",
    "    model = DQN(env.observation_space.shape[0], \n",
    "                 env.action_space.n,\n",
    "                 config['HIDDEN_LAYER_WIDTH'])    \n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "print(model)\n",
    "summary(model, \n",
    "        input_size=(env.observation_space.shape[0],),\n",
    "        batch_size=config['BATCH_SIZE'], \n",
    "        device='cuda' if USE_CUDA else 'cpu' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "if (config['OPTIMIZER']=='Adam'):\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config['LEARNING_RATE'])\n",
    "elif (config['OPTIMIZER']=='SGD'):\n",
    "    optimizer = optim.SGD(model.parameters(), \n",
    "                           lr=config['LEARNING_RATE'])\n",
    "else: #default optimizer is Adam\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config['LEARNING_RATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITERION\n",
    "if (config['CRITERION']=='MSE'):\n",
    "    criterion = nn.MSELoss()\n",
    "elif (config['CRITERION']=='HUBER'):\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "else: #default criterion is MSELoss\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY BUFFER\n",
    "replay_buffer = ReplayBuffer(capacity=config['REPLAY_BUFFER_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target.load_state_dict(model.state_dict())\n",
    "    \n",
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.tensor(np.float32(state)      ,dtype=torch.float32).to(device)\n",
    "    next_state = torch.tensor(np.float32(next_state) ,dtype=torch.float32, requires_grad=False).to(device)\n",
    "    action     = torch.tensor(action                ,dtype=torch.long).to(device)\n",
    "    reward     = torch.tensor(reward                ,dtype=torch.float32).to(device)\n",
    "    done       = torch.tensor(done                  ,dtype=torch.float32).to(device)\n",
    "\n",
    "    q_values = model(state)\n",
    "    q_value  = q_values.gather(dim=1, index=action.unsqueeze(dim=1)).squeeze(dim=1)\n",
    "\n",
    "    #next_q_value\n",
    "    if (config['MODEL_NAME']=='D1QN'):\n",
    "        next_q_values = model(next_state)\n",
    "        next_q_value  = next_q_values.max(dim=1)[0]\n",
    "\n",
    "    elif (config['MODEL_NAME']=='DQN'):\n",
    "        next_q_values = target(next_state)\n",
    "        next_q_value  = next_q_values.max(dim=1)[0]\n",
    "        \n",
    "    elif (config['MODEL_NAME']=='D2QN'):\n",
    "        next_q_values = model(next_state) #all q-values from current model\n",
    "        next_q_target_values = target(next_state) #all q-values from target model\n",
    "        next_q_value = next_q_target_values.gather(dim=1, \n",
    "                                                  index=torch.max(next_q_values, dim=1)[1].unsqueeze(dim=1)).squeeze(dim=1)\n",
    "        #q-values from target model by acting greedily on current model (double dqn)\n",
    "        \n",
    "    else: #Default is D1QN\n",
    "        next_q_values = model(next_state)\n",
    "        next_q_value  = next_q_values.max(dim=1)[0]\n",
    "    \n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "\n",
    "    loss = criterion(q_value, expected_q_value)\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (config['MODEL_NAME']=='DQN' or config['MODEL_NAME']=='D2QN'):\n",
    "    update_target(model, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 25min 58s, sys: 8h 51min 33s, total: 13h 17min 31s\n",
      "Wall time: 1h 12min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training\n",
    "num_frames = config['TIMESTEPS']\n",
    "batch_size = config['BATCH_SIZE']\n",
    "gamma      = config['GAMMA']\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        writer.add_scalar('episode_reward', episode_reward, global_step=frame_idx)\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "        writer.add_scalar('loss', loss.item(), global_step=frame_idx)\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                writer.add_histogram('model_'+ name, param.data, global_step=frame_idx)\n",
    "                \n",
    "        if (config['MODEL_NAME']=='DQN' or config['MODEL_NAME']=='D2QN'):\n",
    "            for name, param in target.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    writer.add_histogram('target_'+ name, param.data, global_step=frame_idx)        \n",
    "    \n",
    "    if (config['MODEL_NAME']=='DQN' or config['MODEL_NAME']=='D2QN'or config['MODEL_NAME']=='D3QN'):\n",
    "        if frame_idx % config['TARGET_UPDATE_FREQ'] == 0:\n",
    "            update_target(model, target)\n",
    "            \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
